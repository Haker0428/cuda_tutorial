# Flash Attention 优化指南

## 📋 代码结构

代码已按关键步骤封装，方便针对每个步骤进行优化：

### 1. `compute_qk_scores_and_max<HEAD_DIM>`
**功能**: 计算 Q @ K^T 分数并找到 block 内的最大值

**当前实现**:
- 使用 `fmaf` 进行点积计算
- 循环计算每个 K 位置的分数

**优化方向**:
- ✅ 使用共享内存缓存 Q 和 K blocks
- ✅ 使用向量化加载（`float4`/`float2`）
- ✅ 使用 Tensor Cores（FP16/BF16）
- ✅ 展开循环以利用指令级并行
- ✅ 使用 warp-level primitives 优化最大值计算

**示例优化**:
```cuda
// 使用共享内存缓存
__shared__ float q_tile[BLOCK_SIZE_Q][HEAD_DIM];
__shared__ float k_tile[BLOCK_SIZE_K][HEAD_DIM];

// 使用向量化加载
float4 q_vec = *reinterpret_cast<float4*>(&q[q_off + d]);
```

---

### 2. `compute_block_softmax_and_weighted_v<HEAD_DIM>`
**功能**: 计算 block 内的 softmax 和加权 V

**当前实现**:
- 使用 `__expf` 计算指数
- 使用 `fmaf` 累加加权 V

**优化方向**:
- ✅ 使用共享内存缓存 V block
- ✅ 向量化 V 的加载和存储
- ✅ 使用 warp shuffle 优化 softmax 计算
- ✅ 使用快速 exp 近似（如果精度允许）
- ✅ 合并内存访问模式

**示例优化**:
```cuda
// Warp-level softmax
float max_val = warpReduceMax(score);
float exp_val = __expf(score - max_val);
float sum = warpReduceSum(exp_val);
```

---

### 3. `merge_block_stats<HEAD_DIM>`
**功能**: 合并 block 统计信息（Online Softmax 核心）

**当前实现**:
- 直接计算缩放因子并合并

**优化方向**:
- ✅ 向量化合并操作（使用 `float4`）
- ✅ 减少寄存器使用
- ✅ 优化 exp 计算（可能可以复用）

**注意**: 这是 Flash Attention 的核心算法，通常不需要大幅修改逻辑。

---

### 4. `normalize_output<HEAD_DIM>`
**功能**: 归一化最终输出

**当前实现**:
- 简单的除法归一化

**优化方向**:
- ✅ 向量化归一化操作
- ✅ 使用快速倒数（`__frcp_rn`）
- ✅ 合并到最终写入操作中

---

## 🚀 整体优化策略

### 内存优化
1. **共享内存使用**
   - 缓存 Q block（如果多个 query 共享）
   - 缓存 K/V blocks
   - 减少全局内存访问

2. **内存访问模式**
   - 合并访问（coalesced access）
   - 使用向量化加载/存储
   - 预取下一个 block

### 计算优化
1. **数值精度**
   - FP16/BF16 使用 Tensor Cores
   - 混合精度计算
   - 快速数学函数（`__expf`, `__frcp_rn`）

2. **并行度**
   - 调整 block 大小
   - 使用 warp-level primitives
   - 优化线程块配置

### 硬件特定优化
1. **Tensor Cores** (V100/A100/H100)
   - 使用 `wmma` API
   - FP16/BF16 矩阵乘法

2. **内存层次**
   - L2 缓存优化
   - 使用 `__ldg` 只读缓存

3. **指令优化**
   - 使用内联函数
   - 减少分支
   - 循环展开

---

## 📊 性能分析工具

### 使用 Nsight Compute 分析
```bash
ncu --set full -o report python test_flash_attention.py
```

### 关键指标
- Memory Throughput
- Compute Throughput
- Warp Efficiency
- Occupancy
- Register Usage

---

## 🔧 配置参数

### 可调参数
- `BLOCK_SIZE_K`: K/V 的块大小（默认 64）
- `BLOCK_SIZE_Q`: Q 的块大小（如果实现 Q 分块）
- Thread block 大小（当前 128）

### 根据硬件调整
- **V100**: BLOCK_SIZE_K = 64, threads = 128
- **A100**: BLOCK_SIZE_K = 128, threads = 256
- **H100**: BLOCK_SIZE_K = 128, threads = 256

---

## 📝 优化检查清单

- [ ] 添加共享内存缓存
- [ ] 实现向量化内存访问
- [ ] 优化 warp-level 操作
- [ ] 使用 Tensor Cores（如果支持）
- [ ] 调整 block 大小
- [ ] 优化寄存器使用
- [ ] 减少分支和循环开销
- [ ] 使用快速数学函数
- [ ] 优化内存访问模式
- [ ] 性能分析和调优

---

## 🔗 参考资源

- [Flash Attention 论文](https://arxiv.org/abs/2205.14135)
- [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [Nsight Compute](https://developer.nvidia.com/nsight-compute)
